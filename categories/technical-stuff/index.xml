<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Technical Stuff on bigelectrons</title><link>https://bigelectrons.com/categories/technical-stuff/</link><description>Recent content in Technical Stuff on bigelectrons</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright Â© 2018 - 2022 @ https://github.com/Joesan</copyright><lastBuildDate>Fri, 26 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://bigelectrons.com/categories/technical-stuff/index.xml" rel="self" type="application/rss+xml"/><item><title>When a Bloom filter really blooms....</title><link>https://bigelectrons.com/post/mlandai/bloom-filter/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/mlandai/bloom-filter/</guid><description>
Definition Simply put, a Bloom filter is a space-efficient probabilistic data structure with which we can determine the probable existence of a certain thing in a certain data set, and we can determine the non-existence of a certain thing in a certain data set with utmost accuracy. Doing all this in a memory space efficient manner.
In a gist, Bloom filters are about determining if an element {{ textcolor(color=&amp;quot;red&amp;quot; text=&amp;quot;may be in a set&amp;quot;) }} or {{ textcolor(color=&amp;quot;red&amp;quot; text=&amp;quot;is definitely not in a set&amp;quot;) }}</description></item><item><title>Keeping up with Upstream Git Repo</title><link>https://bigelectrons.com/post/infra/git-upstream-origin-fork/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/infra/git-upstream-origin-fork/</guid><description>
If you have forked a repository in GitHub that you want to work on, you do so, make your changes locally and once you are confident that everything works, you issue a pull request to the upstream so that your changes can be reviewed and merged. Here is a small snippet that I use to keep my fork up-to-date with the upstream.
upstream - The original repository from which you fork</description></item><item><title>Lamport's Logical Clock - Ordering Events in a Distributed System</title><link>https://bigelectrons.com/post/engineering/lamport-logical-clock-1/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/engineering/lamport-logical-clock-1/</guid><description>
Basically a distributed system is one in which the components or processes or nodes that comprise a system is distributed across nodes or sometimes even across geographies. But these systems need to communicate with each other to accomplish something meaningful. The most efficient, scalable and proven way of making these distributed systems communicate is via passing messages, sometimes even having a messaging middleware (like Kafka for example.,) as a central component.</description></item><item><title>Deeply disappointed with Travis CI and the way OSS is treated</title><link>https://bigelectrons.com/post/infra/travis-ci-1/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/infra/travis-ci-1/</guid><description>
I have been a happy user of the Travis CI free usage over the last couple of years. But one announcement recently made me deeply worried. Have a look here at their announcement
Yes, Travis CI will no longer be free for OSS projects. There is no point in blaming Travis CI for this but rather on those idiots who abused the free usage with crypto minites and tor nodes. These bastards completely broke the trust Travis CI free tier gave for OSS projects.</description></item><item><title>CI for Kubernetes Resources</title><link>https://bigelectrons.com/post/infra/k8s-deployment-ci/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/infra/k8s-deployment-ci/</guid><description>
Recently I have been exploring more around the Kubernetes tooling, especially the ones that can do some pre-validation on the schema and the state of my Kubernetes deployment resources or more precisely the YAML files.
I came across a few of them like conftest, kubeval etc and a wrapper around such set of tools like the kube-tools project which can be used as a GitHub Actions in your GitHub project.</description></item><item><title>GitOps with K8s</title><link>https://bigelectrons.com/post/infra/k8s-git-ops-1/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/infra/k8s-git-ops-1/</guid><description>
Some time around the summer of 2019, I came across a blog post where I read about GitOps and I was flattened by the idea itself and ever since I wanted to give it a try on some serious projects but never had the chance to do it professionally. I had a chat about this with many of my colleagues at work and my team, they seemed ro like the idea but were a bit reluctant to employ this concept in their projects.</description></item><item><title>Understanding Back Propagation Neural Network Architectures</title><link>https://bigelectrons.com/post/math/back-propogation-neural-network/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/back-propogation-neural-network/</guid><description>
Once you understand Feed Forward mechanism which isn't much useful when you do not back propagate to update the weights and refine your Neural Network, your Neural Network isn't of much use. So in this blog post, let us understand what is back propagation and how could we represent the back propagation mathematically!
Back propagation generally means that you distribute the errors to the hidden layers in a proportionate manner relative to how much the weights were contributing to the actual error in the subsequent layer.</description></item><item><title>Understanding Feed Forward Neural Network Architectures</title><link>https://bigelectrons.com/post/math/feed-forward-neural-net/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/feed-forward-neural-net/</guid><description>
I have been reading through the architectures of Neural Networks and wanted to grasp the idea behind calculating the weights in a Neural Network and as you can see in the image below is a simple 2 node 2 layer Neural Network.
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } Web path: https://bigelectrons.com/images/math-feed-forward-neural-network.jpg
Disk path: /static//images/math-feed-forward-neural-network.jpg
Using Page Bundles: false</description></item><item><title>Math behind simple Linear Regression</title><link>https://bigelectrons.com/post/math/linear-regression/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/linear-regression/</guid><description>
I have been wondering on how the math behind a Linear regression works as in most of the ML books that you encounter, the focus will be on giving you a linear equation and just plugging this equation in a Python library to solve for the slope and bias and then use it to predict the new values. It is very rare that they show you how to find the m and b values.</description></item><item><title>CCTV Monitor with Raspberry Pi &amp; OpenCV</title><link>https://bigelectrons.com/post/mlandai/raspi-with-opencv/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/mlandai/raspi-with-opencv/</guid><description>
I wanted to play around with OpenCV and thought it might be a good idea to try OpenCV with a real life use case. DIY'ing a home camera system that can do motion detection and click images when there is some movement in the frame sounded like a cool idea. So I researched on how I could get this set up done.
There were quite a few things that I should decide, like for example.</description></item><item><title>K8s Cluster SetUp on a Set of Raspberry Pi</title><link>https://bigelectrons.com/post/infra/k8s-raspi-cluster/</link><pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/infra/k8s-raspi-cluster/</guid><description>
Some time ago I managed to set up a 4 node K8s cluster on a set of Raspberry Pi's that were lying idle at my home. In case it interests you, please have a look here for the complete setup, the required components and on how to get it up and running</description></item><item><title>Liner Interpolation</title><link>https://bigelectrons.com/post/math/liner-interpolation/</link><pubDate>Wed, 13 Jan 2016 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/liner-interpolation/</guid><description>
In mathematics, linear interpolation is understood as a method of curve fitting using linear polynomials. A polynomial is just an expression that consists of variables along with their co-efficients. An example of a polynomial would be:
1x+2y&amp;amp;sup2 In the above equation, the variable x has 1 as its coefficient, the variable y has 2 as its coefficient. Let us assume that we have a set of linear data points pointing to the x and y-axis:</description></item></channel></rss>