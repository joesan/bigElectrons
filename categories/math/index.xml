<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math on bigelectrons</title><link>bigelectrons.com/categories/math/</link><description>Recent content in math on bigelectrons</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright Â© 2018 - 2022 @ https://github.com/Joesan</copyright><lastBuildDate>Tue, 10 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="bigelectrons.com/categories/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Posts</title><link>bigelectrons.com/post/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>bigelectrons.com/post/</guid><description/></item><item><title>Understanding Back Propagation Neural Network Architectures</title><link>bigelectrons.com/post/math/back-propogation-neural-network/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>bigelectrons.com/post/math/back-propogation-neural-network/</guid><description>
Once you understand Feed Forward mechanism which isn't much useful when you do not back propagate to update the weights and refine your Neural Network, your Neural Network isn't of much use. So in this blog post, let us understand what is back propagation and how could we represent the back propagation mathematically!
Back propagation generally means that you distribute the errors to the hidden layers in a proportionate manner relative to how much the weights were contributing to the actual error in the subsequent layer.</description></item><item><title>Understanding Feed Forward Neural Network Architectures</title><link>bigelectrons.com/post/math/feed-forward-neural-net/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>bigelectrons.com/post/math/feed-forward-neural-net/</guid><description>
I have been reading through the architectures of Neural Networks and wanted to grasp the idea behind calculating the weights in a Neural Network and as you can see in the image below is a simple 2 node 2 layer Neural Network.
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } Web path: /images/math-feed-forward-neural-network.jpg
Disk path: /static//images/math-feed-forward-neural-network.jpg
Using Page Bundles: false</description></item><item><title>Math behind simple Linear Regression</title><link>bigelectrons.com/post/math/linear-regression/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>bigelectrons.com/post/math/linear-regression/</guid><description>
I have been wondering on how the math behind a Linear regression works as in most of the ML books that you encounter, the focus will be on giving you a linear equation and just plugging this equation in a Python library to solve for the slope and bias and then use it to predict the new values. It is very rare that they show you how to find the m and b values.</description></item><item><title>Liner Interpolation</title><link>bigelectrons.com/post/math/liner-interpolation/</link><pubDate>Wed, 13 Jan 2016 00:00:00 +0000</pubDate><guid>bigelectrons.com/post/math/liner-interpolation/</guid><description>
In mathematics, linear interpolation is understood as a method of curve fitting using linear polynomials. A polynomial is just an expression that consists of variables along with their co-efficients. An example of a polynomial would be:
1x+2y&amp;amp;sup2 In the above equation, the variable x has 1 as its coefficient, the variable y has 2 as its coefficient. Let us assume that we have a set of linear data points pointing to the x and y-axis:</description></item></channel></rss>