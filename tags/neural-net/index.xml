<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>neural-net on bigelectrons</title><link>https://bigelectrons.com/tags/neural-net/</link><description>Recent content in neural-net on bigelectrons</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright © 2018 - 2022 @ https://github.com/Joesan</copyright><lastBuildDate>Tue, 10 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://bigelectrons.com/tags/neural-net/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Back Propagation Neural Network Architectures</title><link>https://bigelectrons.com/post/math/back-propogation-neural-network/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/back-propogation-neural-network/</guid><description>
Once you understand Feed Forward mechanism which isn't much useful when you do not back propagate to update the weights and refine your Neural Network, your Neural Network isn't of much use. So in this blog post, let us understand what is back propagation and how could we represent the back propagation mathematically!
Back propagation generally means that you distribute the errors to the hidden layers in a proportionate manner relative to how much the weights were contributing to the actual error in the subsequent layer.</description></item><item><title>Understanding Feed Forward Neural Network Architectures</title><link>https://bigelectrons.com/post/math/feed-forward-neural-net/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/feed-forward-neural-net/</guid><description>
I have been reading through the architectures of Neural Networks and wanted to grasp the idea behind calculating the weights in a Neural Network and as you can see in the image below is a simple 2 node 2 layer Neural Network.
As you can see that for simplicity’s sake, I have just used a 2 node 2 layer network, but the same holds true for any sized Neural Network.</description></item></channel></rss>